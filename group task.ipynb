{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":18,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlptext/lakshmiED.txt\n/kaggle/input/nlpnatgeoact/NLP_NatGeoActivity.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"f = open('../input/nlptext/lakshmiED.txt','r',encoding='utf-8')\ntext = f.read()\ntext = text.lower()","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ntranslate_table = dict((ord(char), None) for char in string.punctuation)   \ntext = text.translate(translate_table)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\ntokenizer.tokenize(text)\nprint(text)","execution_count":21,"outputs":[{"output_type":"stream","text":"i  like a bit of powwow in any place let me rephrase before you think i am eternally hankering for a fight what i mean is i would choose crooked streets over straight highways sweaty mayhem over pristine elegance this is why no matter where i go in this world coming home to india and especially bombay is never dull i blame growing up in the city for my pugilistic predilections one of the many descriptors that mark twain used in relation to bombay was “powwow” the place seemed to confound him “bewitching” “bewildering” “enchanting” “arabian nights come again”—the man was repulsed and riveted at the same time it was a place befitting the number of exclamations he usedat 13 i was yet to be permitted the pleasures of travelling unchaperoned outside bombay but within its confines i had free rein to indulge my inner flâneur i became the weekend loafer slacking through parts of the city i really had no business being in my itinerary hardly ever changed take the best bus to chowpatty after filling up on chaat sample some more at the khau gully in churchgate sometimes pretend to shop for music i could not afford at kala ghoda’s rhythm house where the desperatelytryingtobehip hung out in the 1990s the final stretch was always my favourite trudging along to my personal shangrila victoria terminusat vt i parsed the sea of faces i drummed up mind games to fill time like “who’s new and who isn’t” spotting either was fairly simple the former bunch bears dazed glances and open mouths a person gyrating through the mob with minimum physical contact had been practising for the local train olympics for a few years at least when it was my turn to head back to the suburbs i warmed up adopted a stance that would make usain bolt proud and dashed off like the flash into an incoming train like millions of others bombay taught me independent travel in the crudest sense of the term and it prepared me for the swirling madness that lies in the rest of indiain august we are showcasing and tolling the allure of domestic journeys hampi celebrated for its ruins reveals something unexpected after every visit in hyderabad we feature sufi shrines some of which abound in nooks you wouldn’t notice banaras’s cosy classical music cafés leave a lasting impression on a newcomer’s heart and in west bengal a heritage renaissance seems to be afoot in serampore it is incumbent that travellers make forays far from where they live but every so often it doesn’t hurt to stumble upon surprises in our own backyardanniversary editions have the feel of a graduation a year of studious slogging of which truth be told my team and i do very little and madcap fun which we only wish we could indulge in more rounded off with a sense of achievement and lingering anxiety there’s pride that national geographic traveller india has lived to see another day and in today’s precarious media landscape that should account for something then the gnawing question did we get it right when it comes to travel is there a right or a wrong way to do it early this month the new york times unearthed albert einstein’s entries of his journeys around asia and discovered a surprising side to the nobel prize winner about his time in mainland china he wrote “in the air there is a stench of neverending manifold variety” the people he found were “industrious filthy obtuse…” travel often functions as a rorschach test of biases some are acutely aware of this and spend their time making amends anthony bourdain’s recent passing prompted glowing tributes from around the world to his openminded exploration of parts and cultures unknown there are others who stand their ground if a traveller’s true sentiments veer towards exotification maybe it should stay so read indianamerican author akhil sharma’s recounting of a fortnight in japan featured in this issue for a perfect example the counter to which also in this edition are the observations of three insiders on their hometowns member of parliament shashi tharoor sings paeans to thiruvananthapuram musician raghu dixit toasts mysore and writer janice pariat reminisces about shillong the “how” of travel is a matter of debate too dyedinthewool snobs harp on about authenticity and immersing yourself in local culture the more you are inconvenienced the more real your journey to which casual travellers will respond with “i will take my comfortable stay in a nice hotel thank you very much” ngti’s sixth anniversary is a distillation of these myriad attitudes to travel in their own way our writers show you the “right way to do it” our centrepiece is the “smart hacks” section that features an expert’s take on how best to navigate a place lensman abhishek hajela a regular visitor to ladakh gives readers a glimpse into getting droolworthy shots in ladakh vaishali dinakaran an avowed gearhead has the lowdown on grappling with europe by road kaushal karkhanis decodes solo backpacking in south america for the faraway dreamers chinmai gupta offers a guide through that most “mystical” of institutions—a london nightclub and if these stories are only a reminder of how illprepared your wallet is to go anywhere we have solutions for that too as to whether we got it right we have another year to fuss over that for my money memorable disagreements often centre on food a friend who was about to settle abroad was feeling particularly wistful about a storied south bombay restaurant the kind of eatery that locals like to call “overrated” and guidebooktoting tourists faithfully make a beeline for his favourite on the menu the baklava—a dry fruitladen traditional sweet that smacked of decadence in every bite the first time he requested for the dessert at the restaurant its eccentric owner was not impressed sizing up his credentials he asked “have you had baklava before” “yes” “where” “in turkey” suspicions confirmed the gentleman chided him “arre baba that is the turkish baklava this is the iranian one…” what followed was a 10minute tutorial on the precise ways in which they differed partcomical and partendearingforget the grand battles for identity being waged around the world food drives everyday culture wars they are infinitely more interesting and the only injury caused is to one’s pride—we could all use some schooling on that front besides unlike spikier tiffs these usually end in smiles and a knowing wink last year i was perusing dinner options at le goutillon an unpretentious french bistro in the heart of chantilly after four days in the country most of my companions were satisfied with their fill of meat and wine and chose conservatively however i and another compatriot were feeling emboldened perhaps it was the glass of red and scanned the chalkboard menu for more adventurous fare “bring us the steak” we declared when our substantial cuts of beef arrived it was soon revealed that we had held our appetite in unwarranted esteem at the end of that meal goutillon’s server—a stern nononsense woman—took an eyebrowcocked look at our barely empty dishes and shook her head in disapproval a local hotel manager later reminded us of our misplaced bravado “oh yeah i heard about the indians who didn’t finish their steak” he giggled france 2 india 0 in this time’s food special though india’s showing is strong bombay canteen’s chef thomas zacharias a flagbearer for all things desi picks his top 10 musthaves from across the country a devout traveller he dishes on where you can seek out the finest haleem and unforgettable curries we explore subcultures in delhi kolkata and mumbai through three different food walks writer reema islam pens a heartfelt ode to dolmades the grecian staple that is intertwined with her own history of growing up in libya and bangladesh antoine lewis gathers a list of kitchen maestros from alex atala to a reclusive monk in south korea each of whom are worth a pilgrimage ardent gourmands might also want to consider dubrovnik which in writer david farley’s words is europe’s emerging food capital as you can tell by the examples above we were guided by pure gluttony this timeour yearend edition toasts ultraindulgence while travelling featuring itineraries that many will know to be out of their financial reach in producing these narratives i was struck by a contrast travel today is dominated by minimalists or downsizers those who preach the gospel of “hardknock wanderlust” and they almost always reap universal admiration they are characters to aspire to examples of madeforinstagram sayings such as “all you need is a backpack” or “motorcyclediaries” unable to join these gallivanting philosophers others marvel at their brave rebellion—oh to give up the predictability of overpriced tourist traps someday they sighin this context luxury travel evokes a molotov cocktail of feelings a billionaire on a sailboat hosting jazz agestyle revelries in the french riviera is inevitably setting himself up for mockery the heiress who flits off to shopping holidays in milan and dubai might as well buy an extra pair of sunglasses for the shade directed her way extravagance passes muster if it panders to affordability in the last few years it has become intertwined with entitlement a radioactive pejorative todayupperclass travel doesn’t deserve this slight as more astute aesthetes have reminded us in the past refined tastes don’t have to be gauche living like royalty might have its privileges but it also spurs a temperament for beauty grace and sensuality which is why travellers will always fork out top penny for a night in rajasthan’s many palace stays wealth facilitates the kind of understated exclusivity seen in the english countryside’s several castles or manors once a venue for elegant ballroom dances luxury could also simply mean time well spent—or doing nothing—floating atop a sundeck in an unending stretch of the ocean professional travel writers are lucky to be granted access to these private paradises and in december’s magazine a handful of them have returned with colourful dispatches one writer enjoys a happy recreational bubble in the maldives another is privy to up close views of big game in botswana there is also a roundup of new york’s elite food and drinking haunts and coverage of the maiden cruise between mumbai and goa all these retreats promise a hedonistic binge grand feasts of fine wine and champagne and views hidden from the typical trails some of them will test your pursestrings but think of holly golightly she couldn’t lay claim to real tiffany’s jewels but that never stopped her from getting her heart’s fill standing outside the window\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentence tokenization\nfrom nltk import sent_tokenize, word_tokenize\nsent_tokenize(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#word tokenization\nfor sent in sent_tokenize(text):\n    print(word_tokenize(sent))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing stop-words\nfrom nltk.corpus import stopwords\n\nstopwords_en = stopwords.words('english')\nEN_Stopwords = set(stopwords.words('english')) # Set checking is faster in Python than list.\n# Tokenize and lowercase\ntokenized_lowercase = list(map(str.lower, word_tokenize(text)))\nstopwords_english = set(stopwords.words('english')) # Set checking is faster in Python than list.\nprint([word for word in tokenized_lowercase if word not in stopwords_en])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define punchuation\nfrom string import punctuation\nprint('From string.punctuation:', type(punctuation), punctuation)\npunct_stopwords = stopwords_english.union(set(punctuation))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punch_stop_word= [word for word in tokenized_lowercase if word not in punct_stopwords]\nprint(punch_stop_word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stemming\nfrom nltk.stem import PorterStemmer\n\nporter = PorterStemmer()\n\nfor word in punch_stop_word:\n    print(porter.stem(word))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lemmatization\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\nfor word in punch_stop_word:\n    print(wnl.lemmatize(word))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wnl = WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' # if mapping isn't found, fall back to Noun.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#POS tagging\nfrom nltk import pos_tag\ndef lemmatize_sent(text): \n    # Text input is string, returns lowercased strings.\n    return[wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n            for word, tag in pos_tag(word_tokenize(text))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Raw Text Before Lemmatization ')\nprint(text, '\\n')\nprint('Raw Text After Stop word Removal & Lemmaztization \\n')\nprint([word for word in lemmatize_sent(text) \n       if word not in stopwords_english\n       and not word.isdigit() ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer \ndata = [text,'between a sweep of mountains and an expanse of dark waters a 14story building looms over prince william sound most of whittier alaska’s 280 residents live in the peachcolored confines of begich tower which was built in 1956 as a us army barracks the building has its own post office and grocery store an underground tunnel leads to the town’s small school “we are our own petri dish—we share the same ventilation system” says jim hunt the city’s manager\\n\\nwhen covid19 reached the state controlling visitors was the town’s best hope of keeping the disease at bay—and for months they did there are only two ways to reach whittier by boat or driving the 60 miles from anchorage passing through a singlelane tunnel then in june 11 seasonal seafood processors tested positive for covid19 and departed to isolate in anchorage a month later two more cases appeared among workers at businesses along the harbor finally in august the virus penetrated begich tower an employee who worked in maintenance—which includes covid19 disinfection—tested positive along with five members of his family\\n\\nthe employee chose to get tested in anchorage and there’s no obligation between the two city governments to discuss cases but the busybody nature common to small towns eventually delivered the information to the city manager without the rumor mill hunt might not have known\\n\\n“if you test positive in another community we wouldn’t know” says hunt adding bluntly that “we have no contact tracing—none—outside of anecdotal evidence you need human resources for contact tracing”\\n\\nwhittier’s dilemma may sound extreme but it’s become an alarmingly common problem contact tracing the process of identifying who may have come into contact with a known case of covid19 requires people training funding in atrisk places and time—all resources the united states has been short of devoting during the pandemic'] \ntfidf_vectorizer =TfidfVectorizer(stop_words='english')\ntfidf_feature = tfidf_vectorizer.fit_transform(data) ","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_frame=pd.DataFrame(data = tfidf_feature.todense(), columns=tfidf_vectorizer.get_feature_names()) \ndata_frame \n","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"        10  10minute        11       13   14story      1956    1990s  \\\n0  0.02788   0.02788  0.000000  0.02788  0.000000  0.000000  0.02788   \n1  0.00000   0.00000  0.064243  0.00000  0.064243  0.064243  0.00000   \n\n        280        60  abhishek  ...  writers    wrong    wrote     yeah  \\\n0  0.000000  0.000000   0.02788  ...  0.05576  0.02788  0.02788  0.02788   \n1  0.064243  0.064243   0.00000  ...  0.00000  0.00000  0.00000  0.00000   \n\n      year  yearend    years      yes     york  zacharias  \n0  0.08364  0.02788  0.05576  0.02788  0.05576    0.02788  \n1  0.00000  0.00000  0.00000  0.00000  0.00000    0.00000  \n\n[2 rows x 896 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>10</th>\n      <th>10minute</th>\n      <th>11</th>\n      <th>13</th>\n      <th>14story</th>\n      <th>1956</th>\n      <th>1990s</th>\n      <th>280</th>\n      <th>60</th>\n      <th>abhishek</th>\n      <th>...</th>\n      <th>writers</th>\n      <th>wrong</th>\n      <th>wrote</th>\n      <th>yeah</th>\n      <th>year</th>\n      <th>yearend</th>\n      <th>years</th>\n      <th>yes</th>\n      <th>york</th>\n      <th>zacharias</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.02788</td>\n      <td>0.02788</td>\n      <td>0.000000</td>\n      <td>0.02788</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.02788</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.02788</td>\n      <td>...</td>\n      <td>0.05576</td>\n      <td>0.02788</td>\n      <td>0.02788</td>\n      <td>0.02788</td>\n      <td>0.08364</td>\n      <td>0.02788</td>\n      <td>0.05576</td>\n      <td>0.02788</td>\n      <td>0.05576</td>\n      <td>0.02788</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.064243</td>\n      <td>0.00000</td>\n      <td>0.064243</td>\n      <td>0.064243</td>\n      <td>0.00000</td>\n      <td>0.064243</td>\n      <td>0.064243</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 896 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":" from sklearn.metrics.pairwise import linear_kernel\ncosine_similarities = linear_kernel(data_frame[0:], data_frame[1:]).flatten()\ncosine_similarities[0]","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"0.03626907548006499"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import euclidean_distances\ned=euclidean_distances(data_frame[0:],data_frame[1:])\ned[0]","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"array([1.3883306])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=(0.6<i<=1 for i in cosine_similarities)\nb=(0<=i<2 for i in ed[0])\nc_s=all(a)\ne_d=all(b)\n\nfinal_tuple=(c_s,e_d)\nif all(final_tuple):\n    print(\"selected for the job\")\nelse:\n    print(\"Sorry your not selected\")","execution_count":26,"outputs":[{"output_type":"stream","text":"Sorry your not selected\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CBOW\nfrom __future__ import print_function\nimport collections\nimport math\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nfrom matplotlib import pylab\nfrom six.moves import range\nfrom six.moves.urllib.request import urlretrieve\nfrom sklearn.manifold import TSNE","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary_size = 50000\nf = open('../input/nlptext/lakshmiED.txt','r',encoding='utf-8')\ntext_cbow = f.read()\ntext_cbow = text.lower()\nprint(text_cbow)\ndef build_dataset(text_cbow):\n    count = [['UNK', -1]]\n    count.extend(collections.Counter(text_cbow).most_common(vocabulary_size - 1))\n    dictionary= dict()\n    for word, _ in count:\n        print(word)\n        dictionary[word] = len(dictionary)\n    data = list()\n    unk_count = 0\n    for word in text_cbow:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # dictionary['UNK']\n            unk_count = unk_count + 1\n        data.append(index)\n    count[0][1] = unk_count\n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n    return data, count, dictionary, reverse_dictionary\n\ndata, count, dictionary, reverse_dictionary = build_dataset(text_cbow)\nprint('Most common words (+UNK)', count[:10])\nprint('Sample data', data[:10])\n#del words  # Hint to reduce memory.","execution_count":28,"outputs":[{"output_type":"stream","text":"i  like a bit of powwow in any place let me rephrase before you think i am eternally hankering for a fight what i mean is i would choose crooked streets over straight highways sweaty mayhem over pristine elegance this is why no matter where i go in this world coming home to india and especially bombay is never dull i blame growing up in the city for my pugilistic predilections one of the many descriptors that mark twain used in relation to bombay was “powwow” the place seemed to confound him “bewitching” “bewildering” “enchanting” “arabian nights come again”—the man was repulsed and riveted at the same time it was a place befitting the number of exclamations he usedat 13 i was yet to be permitted the pleasures of travelling unchaperoned outside bombay but within its confines i had free rein to indulge my inner flâneur i became the weekend loafer slacking through parts of the city i really had no business being in my itinerary hardly ever changed take the best bus to chowpatty after filling up on chaat sample some more at the khau gully in churchgate sometimes pretend to shop for music i could not afford at kala ghoda’s rhythm house where the desperatelytryingtobehip hung out in the 1990s the final stretch was always my favourite trudging along to my personal shangrila victoria terminusat vt i parsed the sea of faces i drummed up mind games to fill time like “who’s new and who isn’t” spotting either was fairly simple the former bunch bears dazed glances and open mouths a person gyrating through the mob with minimum physical contact had been practising for the local train olympics for a few years at least when it was my turn to head back to the suburbs i warmed up adopted a stance that would make usain bolt proud and dashed off like the flash into an incoming train like millions of others bombay taught me independent travel in the crudest sense of the term and it prepared me for the swirling madness that lies in the rest of indiain august we are showcasing and tolling the allure of domestic journeys hampi celebrated for its ruins reveals something unexpected after every visit in hyderabad we feature sufi shrines some of which abound in nooks you wouldn’t notice banaras’s cosy classical music cafés leave a lasting impression on a newcomer’s heart and in west bengal a heritage renaissance seems to be afoot in serampore it is incumbent that travellers make forays far from where they live but every so often it doesn’t hurt to stumble upon surprises in our own backyardanniversary editions have the feel of a graduation a year of studious slogging of which truth be told my team and i do very little and madcap fun which we only wish we could indulge in more rounded off with a sense of achievement and lingering anxiety there’s pride that national geographic traveller india has lived to see another day and in today’s precarious media landscape that should account for something then the gnawing question did we get it right when it comes to travel is there a right or a wrong way to do it early this month the new york times unearthed albert einstein’s entries of his journeys around asia and discovered a surprising side to the nobel prize winner about his time in mainland china he wrote “in the air there is a stench of neverending manifold variety” the people he found were “industrious filthy obtuse…” travel often functions as a rorschach test of biases some are acutely aware of this and spend their time making amends anthony bourdain’s recent passing prompted glowing tributes from around the world to his openminded exploration of parts and cultures unknown there are others who stand their ground if a traveller’s true sentiments veer towards exotification maybe it should stay so read indianamerican author akhil sharma’s recounting of a fortnight in japan featured in this issue for a perfect example the counter to which also in this edition are the observations of three insiders on their hometowns member of parliament shashi tharoor sings paeans to thiruvananthapuram musician raghu dixit toasts mysore and writer janice pariat reminisces about shillong the “how” of travel is a matter of debate too dyedinthewool snobs harp on about authenticity and immersing yourself in local culture the more you are inconvenienced the more real your journey to which casual travellers will respond with “i will take my comfortable stay in a nice hotel thank you very much” ngti’s sixth anniversary is a distillation of these myriad attitudes to travel in their own way our writers show you the “right way to do it” our centrepiece is the “smart hacks” section that features an expert’s take on how best to navigate a place lensman abhishek hajela a regular visitor to ladakh gives readers a glimpse into getting droolworthy shots in ladakh vaishali dinakaran an avowed gearhead has the lowdown on grappling with europe by road kaushal karkhanis decodes solo backpacking in south america for the faraway dreamers chinmai gupta offers a guide through that most “mystical” of institutions—a london nightclub and if these stories are only a reminder of how illprepared your wallet is to go anywhere we have solutions for that too as to whether we got it right we have another year to fuss over that for my money memorable disagreements often centre on food a friend who was about to settle abroad was feeling particularly wistful about a storied south bombay restaurant the kind of eatery that locals like to call “overrated” and guidebooktoting tourists faithfully make a beeline for his favourite on the menu the baklava—a dry fruitladen traditional sweet that smacked of decadence in every bite the first time he requested for the dessert at the restaurant its eccentric owner was not impressed sizing up his credentials he asked “have you had baklava before” “yes” “where” “in turkey” suspicions confirmed the gentleman chided him “arre baba that is the turkish baklava this is the iranian one…” what followed was a 10minute tutorial on the precise ways in which they differed partcomical and partendearingforget the grand battles for identity being waged around the world food drives everyday culture wars they are infinitely more interesting and the only injury caused is to one’s pride—we could all use some schooling on that front besides unlike spikier tiffs these usually end in smiles and a knowing wink last year i was perusing dinner options at le goutillon an unpretentious french bistro in the heart of chantilly after four days in the country most of my companions were satisfied with their fill of meat and wine and chose conservatively however i and another compatriot were feeling emboldened perhaps it was the glass of red and scanned the chalkboard menu for more adventurous fare “bring us the steak” we declared when our substantial cuts of beef arrived it was soon revealed that we had held our appetite in unwarranted esteem at the end of that meal goutillon’s server—a stern nononsense woman—took an eyebrowcocked look at our barely empty dishes and shook her head in disapproval a local hotel manager later reminded us of our misplaced bravado “oh yeah i heard about the indians who didn’t finish their steak” he giggled france 2 india 0 in this time’s food special though india’s showing is strong bombay canteen’s chef thomas zacharias a flagbearer for all things desi picks his top 10 musthaves from across the country a devout traveller he dishes on where you can seek out the finest haleem and unforgettable curries we explore subcultures in delhi kolkata and mumbai through three different food walks writer reema islam pens a heartfelt ode to dolmades the grecian staple that is intertwined with her own history of growing up in libya and bangladesh antoine lewis gathers a list of kitchen maestros from alex atala to a reclusive monk in south korea each of whom are worth a pilgrimage ardent gourmands might also want to consider dubrovnik which in writer david farley’s words is europe’s emerging food capital as you can tell by the examples above we were guided by pure gluttony this timeour yearend edition toasts ultraindulgence while travelling featuring itineraries that many will know to be out of their financial reach in producing these narratives i was struck by a contrast travel today is dominated by minimalists or downsizers those who preach the gospel of “hardknock wanderlust” and they almost always reap universal admiration they are characters to aspire to examples of madeforinstagram sayings such as “all you need is a backpack” or “motorcyclediaries” unable to join these gallivanting philosophers others marvel at their brave rebellion—oh to give up the predictability of overpriced tourist traps someday they sighin this context luxury travel evokes a molotov cocktail of feelings a billionaire on a sailboat hosting jazz agestyle revelries in the french riviera is inevitably setting himself up for mockery the heiress who flits off to shopping holidays in milan and dubai might as well buy an extra pair of sunglasses for the shade directed her way extravagance passes muster if it panders to affordability in the last few years it has become intertwined with entitlement a radioactive pejorative todayupperclass travel doesn’t deserve this slight as more astute aesthetes have reminded us in the past refined tastes don’t have to be gauche living like royalty might have its privileges but it also spurs a temperament for beauty grace and sensuality which is why travellers will always fork out top penny for a night in rajasthan’s many palace stays wealth facilitates the kind of understated exclusivity seen in the english countryside’s several castles or manors once a venue for elegant ballroom dances luxury could also simply mean time well spent—or doing nothing—floating atop a sundeck in an unending stretch of the ocean professional travel writers are lucky to be granted access to these private paradises and in december’s magazine a handful of them have returned with colourful dispatches one writer enjoys a happy recreational bubble in the maldives another is privy to up close views of big game in botswana there is also a roundup of new york’s elite food and drinking haunts and coverage of the maiden cruise between mumbai and goa all these retreats promise a hedonistic binge grand feasts of fine wine and champagne and views hidden from the typical trails some of them will test your pursestrings but think of holly golightly she couldn’t lay claim to real tiffany’s jewels but that never stopped her from getting her heart’s fill standing outside the window\nUNK\n \ne\na\nt\ni\no\nn\ns\nr\nh\nl\nd\nu\nm\nc\nf\ng\nw\ny\np\nb\nv\nk\n’\n“\n”\nx\nj\n—\nz\n1\n0\n9\nq\n…\n3\nâ\né\n2\nMost common words (+UNK) [['UNK', 0], (' ', 1773), ('e', 1015), ('a', 784), ('t', 758), ('i', 678), ('o', 610), ('n', 592), ('s', 576), ('r', 574)]\nSample data [5, 1, 1, 11, 5, 23, 2, 1, 3, 1]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import text\nfrom keras.utils import np_utils\nfrom keras.preprocessing import sequence\n\nf = open('../input/nlptext/lakshmiED.txt','r',encoding='utf-8')\ntext_cbow = f.read()\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(text_cbow.split())\nword2id = tokenizer.word_index\n\n# build vocabulary of unique words\nword2id['PAD'] = 0\nid2word = {v:k for k, v in word2id.items()}\nwids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in text_cbow.split()]\n\nvocab_size = len(word2id)\nembed_size = 100\nwindow_size = 2 # context window size\n\nprint('Vocabulary Size:', vocab_size)\nprint('Vocabulary Sample:', list(word2id.items())[:20])","execution_count":29,"outputs":[{"output_type":"stream","text":"Vocabulary Size: 975\nVocabulary Sample: [('the', 1), ('of', 2), ('a', 3), ('in', 4), ('to', 5), ('and', 6), ('for', 7), ('is', 8), ('i', 9), ('that', 10), ('it', 11), ('was', 12), ('”', 13), ('on', 14), ('we', 15), ('this', 16), ('at', 17), ('my', 18), ('are', 19), ('you', 20)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_context_word_pairs(corpus, window_size, vocab_size):\n    context_length = window_size*2\n    for words in corpus:\n        sentence_length = len(words)\n        for index, word in enumerate(words):\n            context_words = []\n            label_word   = []            \n            start = index - window_size\n            end = index + window_size + 1\n            \n            context_words.append([words[i] \n                                 for i in range(start, end) \n                                 if 0 <= i < sentence_length \n                                 and i != index])\n            label_word.append(word)\n\n            x = sequence.pad_sequences(context_words, maxlen=context_length)\n            y = np_utils.to_categorical(label_word, vocab_size)\n            yield (x, y)\n            \n            \n# Test this out for some samples\ni = 0\nfor x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n    if 0 not in x[0]:\n        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n    \n        if i == 10:\n            break\n        i += 1","execution_count":30,"outputs":[{"output_type":"stream","text":"Context (X): ['desperately', 'trying', 'be', 'hip'] -> Target (Y): to\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Lambda\nimport pydot\n\n# build CBOW architecture\ncbow = Sequential()\ncbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\ncbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\ncbow.add(Dense(vocab_size, activation='softmax'))\ncbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n# view model summary\nprint(cbow.summary())","execution_count":31,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 4, 100)            97500     \n_________________________________________________________________\nlambda (Lambda)              (None, 100)               0         \n_________________________________________________________________\ndense (Dense)                (None, 975)               98475     \n=================================================================\nTotal params: 195,975\nTrainable params: 195,975\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(1, 10):\n    loss = 0.\n    i = 0\n    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n        i += 1\n        loss += cbow.train_on_batch(x, y)\n        if i % 1000 == 0:\n            print('Processed {} (context, word) pairs'.format(i))\n\n    print('Epoch:', epoch, '\\tLoss:', loss)\n    print()","execution_count":32,"outputs":[{"output_type":"stream","text":"Processed 1000 (context, word) pairs\nEpoch: 1 \tLoss: 11829.486666679382\n\nProcessed 1000 (context, word) pairs\nEpoch: 2 \tLoss: 11421.518645048141\n\nProcessed 1000 (context, word) pairs\nEpoch: 3 \tLoss: 11344.107621908188\n\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-a4e9efe55a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_context_word_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processed {} (context, word) pairs'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                                     class_weight)\n\u001b[1;32m   1694\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nweights = cbow.get_weights()[0]\nweights = weights[1:]\nprint(weights.shape)\n\npd.DataFrame(weights, index=list(id2word.values())[1:]).head()","execution_count":33,"outputs":[{"output_type":"stream","text":"(974, 100)\n","name":"stdout"},{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"           0         1         2         3         4         5         6   \\\nof  -0.039859  0.031722  0.037864 -0.000482  0.029202 -0.008747 -0.025116   \na   -0.006782 -0.029090 -0.048649 -0.047521  0.014153  0.004207 -0.022612   \nin   0.013534 -0.038594  0.019118 -0.022506  0.041585 -0.032851 -0.038408   \nto   0.009976  0.019553  0.055254 -0.009377  0.011499  0.022682 -0.039262   \nand  0.016959  0.007911  0.035978  0.015448 -0.026042 -0.050464 -0.004636   \n\n           7         8         9   ...        90        91        92  \\\nof  -0.006639 -0.025981 -0.035669  ... -0.029014 -0.015156 -0.038493   \na    0.010009 -0.031868 -0.045851  ... -0.015738  0.005155  0.049641   \nin  -0.043475 -0.030651  0.012283  ...  0.034396 -0.003791 -0.017943   \nto   0.012328 -0.006929  0.029944  ... -0.006402 -0.021100  0.056185   \nand  0.054421 -0.030431  0.014612  ... -0.058729 -0.055791  0.014709   \n\n           93        94        95        96        97        98        99  \nof  -0.009913 -0.036955 -0.012946  0.013972  0.040485 -0.028762 -0.021687  \na   -0.045049  0.025592  0.019384 -0.047905 -0.037763 -0.020364 -0.040313  \nin  -0.038374 -0.028175 -0.029752 -0.010472 -0.012605  0.025047  0.028896  \nto   0.041955  0.033220 -0.019641 -0.000898 -0.033154 -0.002341 -0.026894  \nand -0.025209 -0.040563  0.049814 -0.005201  0.010175 -0.034386 -0.034665  \n\n[5 rows x 100 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>of</th>\n      <td>-0.039859</td>\n      <td>0.031722</td>\n      <td>0.037864</td>\n      <td>-0.000482</td>\n      <td>0.029202</td>\n      <td>-0.008747</td>\n      <td>-0.025116</td>\n      <td>-0.006639</td>\n      <td>-0.025981</td>\n      <td>-0.035669</td>\n      <td>...</td>\n      <td>-0.029014</td>\n      <td>-0.015156</td>\n      <td>-0.038493</td>\n      <td>-0.009913</td>\n      <td>-0.036955</td>\n      <td>-0.012946</td>\n      <td>0.013972</td>\n      <td>0.040485</td>\n      <td>-0.028762</td>\n      <td>-0.021687</td>\n    </tr>\n    <tr>\n      <th>a</th>\n      <td>-0.006782</td>\n      <td>-0.029090</td>\n      <td>-0.048649</td>\n      <td>-0.047521</td>\n      <td>0.014153</td>\n      <td>0.004207</td>\n      <td>-0.022612</td>\n      <td>0.010009</td>\n      <td>-0.031868</td>\n      <td>-0.045851</td>\n      <td>...</td>\n      <td>-0.015738</td>\n      <td>0.005155</td>\n      <td>0.049641</td>\n      <td>-0.045049</td>\n      <td>0.025592</td>\n      <td>0.019384</td>\n      <td>-0.047905</td>\n      <td>-0.037763</td>\n      <td>-0.020364</td>\n      <td>-0.040313</td>\n    </tr>\n    <tr>\n      <th>in</th>\n      <td>0.013534</td>\n      <td>-0.038594</td>\n      <td>0.019118</td>\n      <td>-0.022506</td>\n      <td>0.041585</td>\n      <td>-0.032851</td>\n      <td>-0.038408</td>\n      <td>-0.043475</td>\n      <td>-0.030651</td>\n      <td>0.012283</td>\n      <td>...</td>\n      <td>0.034396</td>\n      <td>-0.003791</td>\n      <td>-0.017943</td>\n      <td>-0.038374</td>\n      <td>-0.028175</td>\n      <td>-0.029752</td>\n      <td>-0.010472</td>\n      <td>-0.012605</td>\n      <td>0.025047</td>\n      <td>0.028896</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>0.009976</td>\n      <td>0.019553</td>\n      <td>0.055254</td>\n      <td>-0.009377</td>\n      <td>0.011499</td>\n      <td>0.022682</td>\n      <td>-0.039262</td>\n      <td>0.012328</td>\n      <td>-0.006929</td>\n      <td>0.029944</td>\n      <td>...</td>\n      <td>-0.006402</td>\n      <td>-0.021100</td>\n      <td>0.056185</td>\n      <td>0.041955</td>\n      <td>0.033220</td>\n      <td>-0.019641</td>\n      <td>-0.000898</td>\n      <td>-0.033154</td>\n      <td>-0.002341</td>\n      <td>-0.026894</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>0.016959</td>\n      <td>0.007911</td>\n      <td>0.035978</td>\n      <td>0.015448</td>\n      <td>-0.026042</td>\n      <td>-0.050464</td>\n      <td>-0.004636</td>\n      <td>0.054421</td>\n      <td>-0.030431</td>\n      <td>0.014612</td>\n      <td>...</td>\n      <td>-0.058729</td>\n      <td>-0.055791</td>\n      <td>0.014709</td>\n      <td>-0.025209</td>\n      <td>-0.040563</td>\n      <td>0.049814</td>\n      <td>-0.005201</td>\n      <td>0.010175</td>\n      <td>-0.034386</td>\n      <td>-0.034665</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 100 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import euclidean_distances\n\n# compute pairwise distance matrix\ndistance_matrix = euclidean_distances(weights)\nprint(distance_matrix.shape)\n\n# view contextually similar words\nsimilar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n                   for search_term in ['place','travel','simple']}\n\nsimilar_words","execution_count":34,"outputs":[{"output_type":"stream","text":"(974, 974)\n","name":"stdout"},{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"{'place': ['cocktail', 'disagreements', 'fun', 'decadence', 'exclusivity'],\n 'travel': ['entries', 'flâneur', 'returned', 'especially', 'tiffany’s'],\n 'simple': ['practising', 'rajasthan’s', 'own', 'disagreements', 'soon']}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#kmeans\nimport os\nimport pandas as pd\nimport numpy as np\ng = open('../input/nlpnatgeoact/NLP_NatGeoActivity.csv')\ndata1 =pd.read_csv(g)\ndf = pd.DataFrame(data1)\ndf","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"        0\n0       5\n1       1\n2       1\n3      11\n4       5\n...    ..\n10572   5\n10573   7\n10574  12\n10575   6\n10576  18\n\n[10577 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10572</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10573</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>10574</th>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>10575</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>10576</th>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n<p>10577 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Content']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = df['Content'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(corpus)\n\ntrue_k = 3\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(X)\n\nprint(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i,:10]:\n        print(' %s' % terms[ind])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}